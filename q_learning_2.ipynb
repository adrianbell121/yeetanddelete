{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9s_QoXmBt9Wr"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "_wvsZL74uMye"
   },
   "outputs": [],
   "source": [
    "class CartPoleEnvironment():\n",
    "\n",
    "    def __init__(self, buckets=(1, 1, 6, 12,)):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.buckets = buckets\n",
    "\n",
    "    def discretize(self, states, boxes):\n",
    "        num_box = [None]*4\n",
    "        #print(states)\n",
    "        for i in range(4):\n",
    "            box = boxes[i]\n",
    "            for j in range(len(box)-1):\n",
    "                if states[i] >= box[j] and states[i] <= box[j+1]:\n",
    "                    num_box[i] = j\n",
    "        return tuple(num_box)\n",
    "    def init_q(self,boxes):\n",
    "        dims = []\n",
    "        action = [-10,10]\n",
    "        for i in range(len(boxes)):\n",
    "            dims.append(len(boxes[i])-1)\n",
    "        dims.append(len(action))\n",
    "        return np.zeros(dims)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "KKXoutUmvD8B"
   },
   "outputs": [],
   "source": [
    "class CartPoleAgent():\n",
    "\n",
    "    def __init__(self, alpha=0.5, epsilon=1, episodes=5_000):\n",
    "        self.em = CartPoleEnvironment()\n",
    "        self.boxes = [[-12,-6,-1,0,1,6,12],[-2.4,-0.8,0.8,2.4],[-np.inf,-50,50,np.inf],[-np.inf,-0.5,0.5,np.inf]]\n",
    "        self.q_table = self.em.init_q(self.boxes)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "\n",
    "    def update_q_value(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        Using Bellman equation, update Q-value based on state-action pair\n",
    "        Q(s, a) <- Q(s, a) + alpha(curr_reward + gamma * max(Q(s', a')) - Q(s, a))\n",
    "        \n",
    "        where max(Q(s', a') is the best future reward, and gamma = 1\n",
    "        \"\"\"\n",
    "        prev_q = self.q_table[state][action]\n",
    "        future_reward = self.best_future_reward(new_state)\n",
    "\n",
    "        self.q_table[state][action] = prev_q + self.alpha * (reward + future_reward - prev_q)\n",
    "        \n",
    "    def update_q_value2(self,state,action,reward,new_state,learn_rate,discount_rate):\n",
    "        future_reward = self.best_future_reward(new_state)\n",
    "        self.q_table[state][action] = learn_rate*(reward + discount_rate*future_reward) -self.q_table[state][action]\n",
    "\n",
    "    def best_future_reward(self, state):\n",
    "        return np.max(self.q_table[state])\n",
    "\n",
    "    def choose_action(self, state, epsilon=True):\n",
    "        \"\"\"\n",
    "        Action is chosen using epsilon-greedy algorithm\n",
    "        \"\"\"\n",
    "        best_action = np.argmax(self.q_table[state])\n",
    "        random_action = self.em.env.action_space.sample()\n",
    "\n",
    "        if epsilon:\n",
    "            if random.random() > 1-self.epsilon:\n",
    "                return best_action\n",
    "            else:\n",
    "                return random_action\n",
    "        else:\n",
    "            return best_action\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train for 5,000 episodes where at each episode the exploration is decayed.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        MAX_STEPS = 200\n",
    "        MAX_EXP_RATE = 1\n",
    "        MIN_EXP_RATE = 0.01\n",
    "        EXP_DECAY_RATE = 0.001\n",
    "        learn_rate0 = 0.05\n",
    "        learn_rate_decay = 0.95\n",
    "        exploration_decay_rate=0.99\n",
    "        for ep in range(self.episodes*3):\n",
    "            env = self.em.env\n",
    "            state = self.em.discretize(env.reset(),self.boxes)\n",
    "            self.epsilon = self.epsilon*exploration_decay_rate#MIN_EXP_RATE + (MAX_EXP_RATE - MIN_EXP_RATE) * np.exp(-EXP_DECAY_RATE * ep)\n",
    "            done = False\n",
    "            episode_rewards = 0\n",
    "            step = 0\n",
    "            gamma = 0.95\n",
    "            gamma0 = gamma\n",
    "            while not done and step < MAX_STEPS:\n",
    "\n",
    "                action = self.choose_action(state,self.epsilon)\n",
    "\n",
    "                # take action\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                new_state = self.em.discretize(new_state,self.boxes)\n",
    "           \n",
    "                # accummulate rewards\n",
    "                reward = reward*gamma0\n",
    "                episode_rewards += reward\n",
    "                gamma0 = gamma*gamma0\n",
    "                # update Q-table \n",
    "                learn_rate = learn_rate0/(1+step*learn_rate_decay)\n",
    "                self.update_q_value2(state, action, reward, new_state,learn_rate,gamma0)\n",
    "\n",
    "                # transition to the new state\n",
    "                state = new_state\n",
    "\n",
    "                step += 1\n",
    "\n",
    "            if done: \n",
    "                print(f\"Episode {ep} finished after {step + 1} timesteps.\")\n",
    "            \n",
    "            rewards.append(episode_rewards)\n",
    "        \n",
    "        rewards_per_thousand_ep = np.split(np.array(rewards), self.episodes/500)\n",
    "        count = 500\n",
    "\n",
    "        for r in rewards_per_thousand_ep:\n",
    "            print(f\"{count}: {int(sum(r/500))}\")\n",
    "            count += 500\n",
    "\n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Playing using the populated Q-table; we want to exploit the Q-values.\n",
    "        So we will not use epsilon-greedy algorithm and only select the max Q-value.\n",
    "        \"\"\"\n",
    "        env = self.em.env\n",
    "        env._max_episode_steps = 1000\n",
    "        state = self.em.discretize(env.reset(),self.boxes)\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        while not done:\n",
    "            action = self.choose_action(state, epsilon=False)\n",
    "\n",
    "            # take action\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            new_state = self.em.discretize(new_state,self.boxes)\n",
    "            rewards += reward         \n",
    "\n",
    "            # transition to the new state\n",
    "            state = new_state\n",
    "\n",
    "        print(f\"Agent finished with a reward of {rewards}\")\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yrbjerflwQk5",
    "outputId": "f74cf3d9-7473-42f7-fdee-f88de0ab52e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 13 timesteps.\n",
      "Episode 1 finished after 21 timesteps.\n",
      "Episode 2 finished after 19 timesteps.\n",
      "Episode 3 finished after 21 timesteps.\n",
      "Episode 4 finished after 39 timesteps.\n",
      "Episode 5 finished after 23 timesteps.\n",
      "Episode 6 finished after 57 timesteps.\n",
      "Episode 7 finished after 11 timesteps.\n",
      "Episode 8 finished after 14 timesteps.\n",
      "Episode 9 finished after 10 timesteps.\n",
      "Episode 10 finished after 11 timesteps.\n",
      "Episode 11 finished after 21 timesteps.\n",
      "Episode 12 finished after 13 timesteps.\n",
      "Episode 13 finished after 17 timesteps.\n",
      "Episode 14 finished after 9 timesteps.\n",
      "Episode 15 finished after 15 timesteps.\n",
      "Episode 16 finished after 12 timesteps.\n",
      "Episode 17 finished after 10 timesteps.\n",
      "Episode 18 finished after 9 timesteps.\n",
      "Episode 19 finished after 18 timesteps.\n",
      "Episode 20 finished after 14 timesteps.\n",
      "Episode 21 finished after 11 timesteps.\n",
      "Episode 22 finished after 13 timesteps.\n",
      "Episode 23 finished after 16 timesteps.\n",
      "Episode 24 finished after 14 timesteps.\n",
      "Episode 25 finished after 16 timesteps.\n",
      "Episode 26 finished after 18 timesteps.\n",
      "Episode 27 finished after 16 timesteps.\n",
      "Episode 28 finished after 26 timesteps.\n",
      "Episode 29 finished after 14 timesteps.\n",
      "Episode 30 finished after 25 timesteps.\n",
      "Episode 31 finished after 10 timesteps.\n",
      "Episode 32 finished after 39 timesteps.\n",
      "Episode 33 finished after 13 timesteps.\n",
      "Episode 34 finished after 15 timesteps.\n",
      "Episode 35 finished after 17 timesteps.\n",
      "Episode 36 finished after 11 timesteps.\n",
      "Episode 37 finished after 10 timesteps.\n",
      "Episode 38 finished after 15 timesteps.\n",
      "Episode 39 finished after 11 timesteps.\n",
      "Episode 40 finished after 17 timesteps.\n",
      "Episode 41 finished after 21 timesteps.\n",
      "Episode 42 finished after 41 timesteps.\n",
      "Episode 43 finished after 20 timesteps.\n",
      "Episode 44 finished after 13 timesteps.\n",
      "Episode 45 finished after 9 timesteps.\n",
      "Episode 46 finished after 18 timesteps.\n",
      "Episode 47 finished after 14 timesteps.\n",
      "Episode 48 finished after 22 timesteps.\n",
      "Episode 49 finished after 11 timesteps.\n",
      "Episode 50 finished after 11 timesteps.\n",
      "Episode 51 finished after 10 timesteps.\n",
      "Episode 52 finished after 11 timesteps.\n",
      "Episode 53 finished after 18 timesteps.\n",
      "Episode 54 finished after 14 timesteps.\n",
      "Episode 55 finished after 16 timesteps.\n",
      "Episode 56 finished after 13 timesteps.\n",
      "Episode 57 finished after 23 timesteps.\n",
      "Episode 58 finished after 34 timesteps.\n",
      "Episode 59 finished after 14 timesteps.\n",
      "Episode 60 finished after 20 timesteps.\n",
      "Episode 61 finished after 21 timesteps.\n",
      "Episode 62 finished after 65 timesteps.\n",
      "Episode 63 finished after 12 timesteps.\n",
      "Episode 64 finished after 24 timesteps.\n",
      "Episode 65 finished after 23 timesteps.\n",
      "Episode 66 finished after 13 timesteps.\n",
      "Episode 67 finished after 16 timesteps.\n",
      "Episode 68 finished after 14 timesteps.\n",
      "Episode 69 finished after 13 timesteps.\n",
      "Episode 70 finished after 34 timesteps.\n",
      "Episode 71 finished after 21 timesteps.\n",
      "Episode 72 finished after 12 timesteps.\n",
      "Episode 73 finished after 11 timesteps.\n",
      "Episode 74 finished after 12 timesteps.\n",
      "Episode 75 finished after 16 timesteps.\n",
      "Episode 76 finished after 33 timesteps.\n",
      "Episode 77 finished after 13 timesteps.\n",
      "Episode 78 finished after 21 timesteps.\n",
      "Episode 79 finished after 15 timesteps.\n",
      "Episode 80 finished after 16 timesteps.\n",
      "Episode 81 finished after 12 timesteps.\n",
      "Episode 82 finished after 9 timesteps.\n",
      "Episode 83 finished after 14 timesteps.\n",
      "Episode 84 finished after 23 timesteps.\n",
      "Episode 85 finished after 18 timesteps.\n",
      "Episode 86 finished after 20 timesteps.\n",
      "Episode 87 finished after 50 timesteps.\n",
      "Episode 88 finished after 24 timesteps.\n",
      "Episode 89 finished after 30 timesteps.\n",
      "Episode 90 finished after 35 timesteps.\n",
      "Episode 91 finished after 15 timesteps.\n",
      "Episode 92 finished after 43 timesteps.\n",
      "Episode 93 finished after 10 timesteps.\n",
      "Episode 94 finished after 11 timesteps.\n",
      "Episode 95 finished after 17 timesteps.\n",
      "Episode 96 finished after 11 timesteps.\n",
      "Episode 97 finished after 15 timesteps.\n",
      "Episode 98 finished after 18 timesteps.\n",
      "Episode 99 finished after 30 timesteps.\n",
      "Episode 100 finished after 17 timesteps.\n",
      "Episode 101 finished after 10 timesteps.\n",
      "Episode 102 finished after 21 timesteps.\n",
      "Episode 103 finished after 11 timesteps.\n",
      "Episode 104 finished after 47 timesteps.\n",
      "Episode 105 finished after 26 timesteps.\n",
      "Episode 106 finished after 23 timesteps.\n",
      "Episode 107 finished after 12 timesteps.\n",
      "Episode 108 finished after 18 timesteps.\n",
      "Episode 109 finished after 31 timesteps.\n",
      "Episode 110 finished after 15 timesteps.\n",
      "Episode 111 finished after 27 timesteps.\n",
      "Episode 112 finished after 19 timesteps.\n",
      "Episode 113 finished after 12 timesteps.\n",
      "Episode 114 finished after 32 timesteps.\n",
      "Episode 115 finished after 20 timesteps.\n",
      "Episode 116 finished after 25 timesteps.\n",
      "Episode 117 finished after 34 timesteps.\n",
      "Episode 118 finished after 14 timesteps.\n",
      "Episode 119 finished after 11 timesteps.\n",
      "Episode 120 finished after 16 timesteps.\n",
      "Episode 121 finished after 17 timesteps.\n",
      "Episode 122 finished after 16 timesteps.\n",
      "Episode 123 finished after 14 timesteps.\n",
      "Episode 124 finished after 10 timesteps.\n",
      "Episode 125 finished after 18 timesteps.\n",
      "Episode 126 finished after 13 timesteps.\n",
      "Episode 127 finished after 18 timesteps.\n",
      "Episode 128 finished after 31 timesteps.\n",
      "Episode 129 finished after 16 timesteps.\n",
      "Episode 130 finished after 27 timesteps.\n",
      "Episode 131 finished after 29 timesteps.\n",
      "Episode 132 finished after 39 timesteps.\n",
      "Episode 133 finished after 15 timesteps.\n",
      "Episode 134 finished after 16 timesteps.\n",
      "Episode 135 finished after 15 timesteps.\n",
      "Episode 136 finished after 14 timesteps.\n",
      "Episode 137 finished after 21 timesteps.\n",
      "Episode 138 finished after 12 timesteps.\n",
      "Episode 139 finished after 15 timesteps.\n",
      "Episode 140 finished after 39 timesteps.\n",
      "Episode 141 finished after 18 timesteps.\n",
      "Episode 142 finished after 23 timesteps.\n",
      "Episode 143 finished after 12 timesteps.\n",
      "Episode 144 finished after 13 timesteps.\n",
      "Episode 145 finished after 20 timesteps.\n",
      "Episode 146 finished after 15 timesteps.\n",
      "Episode 147 finished after 29 timesteps.\n",
      "Episode 148 finished after 21 timesteps.\n",
      "Episode 149 finished after 10 timesteps.\n",
      "Episode 150 finished after 17 timesteps.\n",
      "Episode 151 finished after 18 timesteps.\n",
      "Episode 152 finished after 35 timesteps.\n",
      "Episode 153 finished after 11 timesteps.\n",
      "Episode 154 finished after 31 timesteps.\n",
      "Episode 155 finished after 14 timesteps.\n",
      "Episode 156 finished after 12 timesteps.\n",
      "Episode 157 finished after 12 timesteps.\n",
      "Episode 158 finished after 29 timesteps.\n",
      "Episode 159 finished after 64 timesteps.\n",
      "Episode 160 finished after 24 timesteps.\n",
      "Episode 161 finished after 19 timesteps.\n",
      "Episode 162 finished after 29 timesteps.\n",
      "Episode 163 finished after 42 timesteps.\n",
      "Episode 164 finished after 61 timesteps.\n",
      "Episode 165 finished after 15 timesteps.\n",
      "Episode 166 finished after 15 timesteps.\n",
      "Episode 167 finished after 18 timesteps.\n",
      "Episode 168 finished after 14 timesteps.\n",
      "Episode 169 finished after 29 timesteps.\n",
      "Episode 170 finished after 26 timesteps.\n",
      "Episode 171 finished after 51 timesteps.\n",
      "Episode 172 finished after 10 timesteps.\n",
      "Episode 173 finished after 17 timesteps.\n",
      "Episode 174 finished after 15 timesteps.\n",
      "Episode 175 finished after 12 timesteps.\n",
      "Episode 176 finished after 14 timesteps.\n",
      "Episode 177 finished after 69 timesteps.\n",
      "Episode 178 finished after 32 timesteps.\n",
      "Episode 179 finished after 14 timesteps.\n",
      "Episode 180 finished after 23 timesteps.\n",
      "Episode 181 finished after 54 timesteps.\n",
      "Episode 182 finished after 11 timesteps.\n",
      "Episode 183 finished after 28 timesteps.\n",
      "Episode 184 finished after 19 timesteps.\n",
      "Episode 185 finished after 19 timesteps.\n",
      "Episode 186 finished after 16 timesteps.\n",
      "Episode 187 finished after 24 timesteps.\n",
      "Episode 188 finished after 12 timesteps.\n",
      "Episode 189 finished after 21 timesteps.\n",
      "Episode 190 finished after 19 timesteps.\n",
      "Episode 191 finished after 12 timesteps.\n",
      "Episode 192 finished after 15 timesteps.\n",
      "Episode 193 finished after 15 timesteps.\n",
      "Episode 194 finished after 22 timesteps.\n",
      "Episode 195 finished after 22 timesteps.\n",
      "Episode 196 finished after 35 timesteps.\n",
      "Episode 197 finished after 38 timesteps.\n",
      "Episode 198 finished after 44 timesteps.\n",
      "Episode 199 finished after 26 timesteps.\n",
      "Episode 200 finished after 21 timesteps.\n",
      "Episode 201 finished after 12 timesteps.\n",
      "Episode 202 finished after 27 timesteps.\n",
      "Episode 203 finished after 25 timesteps.\n",
      "Episode 204 finished after 14 timesteps.\n",
      "Episode 205 finished after 43 timesteps.\n",
      "Episode 206 finished after 23 timesteps.\n",
      "Episode 207 finished after 18 timesteps.\n",
      "Episode 208 finished after 25 timesteps.\n",
      "Episode 209 finished after 19 timesteps.\n",
      "Episode 210 finished after 13 timesteps.\n",
      "Episode 211 finished after 40 timesteps.\n",
      "Episode 212 finished after 12 timesteps.\n",
      "Episode 213 finished after 26 timesteps.\n",
      "Episode 214 finished after 25 timesteps.\n",
      "Episode 215 finished after 22 timesteps.\n",
      "Episode 216 finished after 40 timesteps.\n",
      "Episode 217 finished after 16 timesteps.\n",
      "Episode 218 finished after 27 timesteps.\n",
      "Episode 219 finished after 21 timesteps.\n",
      "Episode 220 finished after 12 timesteps.\n",
      "Episode 221 finished after 17 timesteps.\n",
      "Episode 222 finished after 21 timesteps.\n",
      "Episode 223 finished after 16 timesteps.\n",
      "Episode 224 finished after 13 timesteps.\n",
      "Episode 225 finished after 36 timesteps.\n",
      "Episode 226 finished after 39 timesteps.\n",
      "Episode 227 finished after 17 timesteps.\n",
      "Episode 228 finished after 14 timesteps.\n",
      "Episode 229 finished after 46 timesteps.\n",
      "Episode 230 finished after 30 timesteps.\n",
      "Episode 231 finished after 10 timesteps.\n",
      "Episode 232 finished after 20 timesteps.\n",
      "Episode 233 finished after 18 timesteps.\n",
      "Episode 234 finished after 14 timesteps.\n",
      "Episode 235 finished after 46 timesteps.\n",
      "Episode 236 finished after 24 timesteps.\n",
      "Episode 237 finished after 23 timesteps.\n",
      "Episode 238 finished after 19 timesteps.\n",
      "Episode 239 finished after 29 timesteps.\n",
      "Episode 240 finished after 16 timesteps.\n",
      "Episode 241 finished after 12 timesteps.\n",
      "Episode 242 finished after 14 timesteps.\n",
      "Episode 243 finished after 13 timesteps.\n",
      "Episode 244 finished after 67 timesteps.\n",
      "Episode 245 finished after 38 timesteps.\n",
      "Episode 246 finished after 40 timesteps.\n",
      "Episode 247 finished after 21 timesteps.\n",
      "Episode 248 finished after 12 timesteps.\n",
      "Episode 249 finished after 19 timesteps.\n",
      "Episode 250 finished after 15 timesteps.\n",
      "Episode 251 finished after 27 timesteps.\n",
      "Episode 252 finished after 17 timesteps.\n",
      "Episode 253 finished after 26 timesteps.\n",
      "Episode 254 finished after 13 timesteps.\n",
      "Episode 255 finished after 16 timesteps.\n",
      "Episode 256 finished after 21 timesteps.\n",
      "Episode 257 finished after 52 timesteps.\n",
      "Episode 258 finished after 17 timesteps.\n",
      "Episode 259 finished after 14 timesteps.\n",
      "Episode 260 finished after 12 timesteps.\n",
      "Episode 261 finished after 10 timesteps.\n",
      "Episode 262 finished after 11 timesteps.\n",
      "Episode 263 finished after 20 timesteps.\n",
      "Episode 264 finished after 19 timesteps.\n",
      "Episode 265 finished after 13 timesteps.\n",
      "Episode 266 finished after 26 timesteps.\n",
      "Episode 267 finished after 14 timesteps.\n",
      "Episode 268 finished after 19 timesteps.\n",
      "Episode 269 finished after 15 timesteps.\n",
      "Episode 270 finished after 28 timesteps.\n",
      "Episode 271 finished after 20 timesteps.\n",
      "Episode 272 finished after 28 timesteps.\n",
      "Episode 273 finished after 24 timesteps.\n",
      "Episode 274 finished after 12 timesteps.\n",
      "Episode 275 finished after 21 timesteps.\n",
      "Episode 276 finished after 16 timesteps.\n",
      "Episode 277 finished after 12 timesteps.\n",
      "Episode 278 finished after 11 timesteps.\n",
      "Episode 279 finished after 39 timesteps.\n",
      "Episode 280 finished after 19 timesteps.\n",
      "Episode 281 finished after 22 timesteps.\n",
      "Episode 282 finished after 15 timesteps.\n",
      "Episode 283 finished after 37 timesteps.\n",
      "Episode 284 finished after 54 timesteps.\n",
      "Episode 285 finished after 26 timesteps.\n",
      "Episode 286 finished after 37 timesteps.\n",
      "Episode 287 finished after 12 timesteps.\n",
      "Episode 288 finished after 26 timesteps.\n",
      "Episode 289 finished after 15 timesteps.\n",
      "Episode 290 finished after 16 timesteps.\n",
      "Episode 291 finished after 14 timesteps.\n",
      "Episode 292 finished after 20 timesteps.\n",
      "Episode 293 finished after 20 timesteps.\n",
      "Episode 294 finished after 19 timesteps.\n",
      "Episode 295 finished after 21 timesteps.\n",
      "Episode 296 finished after 17 timesteps.\n",
      "Episode 297 finished after 16 timesteps.\n",
      "Episode 298 finished after 26 timesteps.\n",
      "Episode 299 finished after 16 timesteps.\n",
      "Episode 300 finished after 15 timesteps.\n",
      "Episode 301 finished after 16 timesteps.\n",
      "Episode 302 finished after 79 timesteps.\n",
      "Episode 303 finished after 29 timesteps.\n",
      "Episode 304 finished after 25 timesteps.\n",
      "Episode 305 finished after 36 timesteps.\n",
      "Episode 306 finished after 14 timesteps.\n",
      "Episode 307 finished after 12 timesteps.\n",
      "Episode 308 finished after 27 timesteps.\n",
      "Episode 309 finished after 29 timesteps.\n",
      "Episode 310 finished after 38 timesteps.\n",
      "Episode 311 finished after 28 timesteps.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-6c004fcd228a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartPoleAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-29b25a638d88>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# update Q-table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_rate0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlearn_rate_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_value2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# transition to the new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-29b25a638d88>\u001b[0m in \u001b[0;36mupdate_q_value2\u001b[0;34m(self, state, action, reward, new_state, learn_rate, discount_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_q_value2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mfuture_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_future_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfuture_reward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbest_future_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "agent = CartPoleAgent()\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNqI-c9G0m-5",
    "outputId": "8d5b6a54-496a-4f56-ad65-4b9d37e377ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent finished with a reward of 9.0\n"
     ]
    }
   ],
   "source": [
    "agent.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
